---
layout: article
title: PCA原理及代码
lang: zh
mathjax: true
---
## PCA原理介绍

PCA全称主成分分析，是最经典的 **无监督** 降维方法之一，其主要原理是将特征进行基变换，使得映射后的特征能够达到协方差为0，方差最大化，其目的在于：

- 协方差为0：协方差不为0，代表特征间具有相关关系，可以相互推断，即存在信息存储的冗余

- 方差最大化：方差最大化的方向投影代表着信息量损耗最小，利于分类

因此，我们的目标在于寻找一个映射矩阵 $P$，使得$PX$的协方差矩阵对角化，同时对角线上的值最大，也就是迹最大。听起来很熟悉有没有，脑海中可以瞬间出现两个方法：特征值分解，SVD。至此我们的优化目标已经明确：**将一组 N 维向量降为 K 维，其目标是选择 K 个单位正交基，使得原始数据变换到这组基上后，各变量两两间协方差为 0，而变量方差则尽可能大（在正交的约束下，取最大的 K 个方差）。**

让我们再捋一捋整个求解的脉络： 

### 1. 方差与协方差

方差：

![[公式]](https://www.zhihu.com/equation?tex=Var%28a%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7B%28a_i-%5Cmu%29%5E2%7D+%5C%5C)

进行0均值处理（均值的变换也就是平移变换并不影响方差，对我们的目的不造成影响）

![[公式]](https://www.zhihu.com/equation?tex=Var%28a%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_i%5E2%7D+%5C%5C)

协方差：

![[公式]](https://www.zhihu.com/equation?tex=Cov%28a%2Cb%29%3D%5Cfrac%7B1%7D%7Bm-1%7D%5Csum_%7Bi%3D1%7D%5Em%7B%28a_i-%5Cmu_a%29%28b_i-%5Cmu_b%29%7D+%5C%5C)

同样是0均值：

![[公式]](https://www.zhihu.com/equation?tex=Cov%28a%2Cb%29%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_ib_i%7D+%5C%5C)

   假设我们只有 a 和 b 两个变量，那么我们将它们按行组成矩阵 $X$：

![[公式]](https://www.zhihu.com/equation?tex=X%3D%5Cbegin%7Bpmatrix%7D++a_1+%26+a_2+%26+%5Ccdots+%26+a_m+%5C%5C+b_1+%26+b_2+%26+%5Ccdots+%26+b_m++%5Cend%7Bpmatrix%7D+%5C%5C)

可以发现：

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bm%7DXX%5E%5Cmathsf%7BT%7D%3D+%5Cbegin%7Bpmatrix%7D++%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_i%5E2%7D+%26+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_ib_i%7D+%5C%5C+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Ba_ib_i%7D+%26+%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7Bb_i%5E2%7D++%5Cend%7Bpmatrix%7D+%3D+%5Cbegin%7Bpmatrix%7D++Cov%28a%2Ca%29+%26+Cov%28a%2Cb%29+%5C%5C++Cov%28b%2Ca%29+%26+Cov%28b%2Cb%29+%5Cend%7Bpmatrix%7D+%5C%5C)

这就是我们想要的原始数据的协方差矩阵，当然，通常我们使用的$X$是转置的，因为习惯了行为样本，列为特征的表示，这点在之后的代码里也有体现。

### 2. 特征值分解

我们很熟悉的特征值分解的公式为：

$$
Ax=\lambda x
$$

也就是说，$A$对$x$起到的只有拉伸作用，没有映射作用，不存在向量方向的改变，而$\lambda$越大，代表$A$的拉长作用越强。那如果好几个特征向量$x_i$组成了一个矩阵，是不是可以得到

$$
AX=[Ax_1,Ax_2...Ax_k]=[\lambda_1x_1,\lambda_2x_2...\lambda_kx_k]
$$

那我们又都知道，协方差矩阵是实对称矩阵，当然也是半正定的，实对称的性质就代表着一定会有$n$个线性无关的特征向量，也就是正交，那我们在前面再乘一个$X^T$，就得到了一个对角矩阵呢，并且由于正交，除了对角线上的元素都是0，对角线上的元素就是特征值。

$$
   X^TAX=\Lambda= \left\{ \begin{matrix} \lambda_1 && & \\  & \lambda_2 &  &   \\  &  & \ddots &    \\  &    &  & \lambda_k \end{matrix} \right\}
$$

所以现在的目标变成了找特征值，从大到小排出$k$个，这就是我们想要降成的维数，然后取出特征向量，垒起来，乘以原矩阵，就是我们最后PCA得到的数据啦，当然，之前的减去均值处理，在这里也无需复原，毕竟只是我们为了简化方差公式的trick，减不减均值都一样的，不影响推导。

关于如何使用SVD进行求解，之后会专门开一章讲SVD，不过也大同小异，一边的奇异向量可以进行维度规约，而另一边的可以做样本筛选罢了，降维得到的结果和特征值分解是一样的。

### 3.使用相关系数矩阵代替协方差矩阵

看到这个标题估计就有想法了，相关系数比协方差多了个啥？多除了两个标准差！加上上面的减均值，也就是我们对原数据进行了一次 **z-score** 标准化，对标准化有概念的大爷们一听就知道：去除量纲影响！当然，对于这个做法我是颇有微词的，这样做其实也一定程度上去除了方差的信息，全部变成0了哪来的信息？

### 4.主成分的贡献率与主成分对变量的贡献率

前者很好理解，代表了主成分代表了原数据多少信息，公式为

$$
\frac{\lambda_k}{Tr(X^TX)}
$$

信息论的角度上，特征值的大小其实就代表信息量的大小，因为PCA的思想就是把原数据$x_1,x_2...x_p$的方差之和重新分配给互不相关的变量$y_1,y_2...y_p$的方差之和，变换后对角线上的元素$\lambda_k$其实就是$y_k$的方差。也就是说，原有的信息量总和（方差和）被变换后某一维的信息量（方差，特征值）解释的程度（比例），叫做贡献率。当然这个我也不是很懂，之后肯定会开一章讲信息论的，毕竟之后想走表征学习这块。

![image-20210527034235306](\pca1.png)

后者代表主成分$y_k$包含了原来的一个变量$x_i$的多少信息，用$y_k$与$x_i$的相关系数$\rho(x_i,y_k)$表示，简单来说就是混血儿的血统浓度吧，毕竟$y_k$是$x_i$的线性组合，这个应该好理解，后面代码里也有做。

![image-20210527034142866](\pca2.png)

### 5.others

首先，PCA是否需要权衡，或者说权衡的效果如何，还需要继续思考。这里的权衡当然指的是**协方差矩阵**与**相关系数矩阵**的冲突，前者可以依照方差信息，但有可能选取一些贡献率较高的主成分时，会忽略一些方差低的变量，而后者没有这个问题。当然根据社达的想法这样的变量本就不该存在（笑），但是量纲的影响不得不考虑，所以有没有可能既去除量纲差异又保留方差差异呢。

其次，前面既然提到了优化问题，我们可不可以用优化的形式把PCA问题表示出来呢？当然可以，参见<https://zhuanlan.zhihu.com/p/77151308> ,在这个问题上拉格朗日乘子法和特征值分解法居然形式一致，只能感叹数学之美。其实我比较偏向拉格朗日啦，毕竟直观好多。



## PCA代码实现

### 共有四个函数，fit,ctbr_single,ctbr,ctbrmat,分别代表着降维，单个主成分的贡献率，降维成k后所有主成分的贡献率，主成分y对原有属性x的贡献值矩阵


```python
import numpy as np
import pandas as pd
import scipy.linalg as lina
import openpyxl
from sklearn.decomposition import PCA as sklearnPCA
class PCA():
    def __init__(self,k,x,pcatype='cov'):
        self.k=k
        self.x=x
        self.pcatype=pcatype
    def fit(self):
        m, n = np.shape(self.x)
        if self.k > n:
            raise ValueError("k must lower than n")
        else:
            def meanX(dataX):
                return np.mean(dataX, axis=0)

            if self.pcatype == 'cov':
                self.x = self.x.values
                average = meanX(self.x)
                data_adjust = self.x - average
                self.covX = data_adjust.T.dot(data_adjust)  # 计算协方差矩阵
                self.featValue, self.featVec = np.linalg.eig(self.covX)  # 求解协方差矩阵的特征值和特征向量
                index = np.argsort(-self.featValue)  # 依照featValue进行从大到小排序
                selectVec = np.matrix(self.featVec[:, index[:self.k]])
                finalData = data_adjust * selectVec  # 正常pca

            if self.pcatype == 'coeff':
                self.x = self.x.values
                average = meanX(self.x)
                data_adjust = self.x - average
                self.covX = np.corrcoef(data_adjust, rowvar=False)
                # print(covX)# 计算协方差矩阵
                self.featValue, self.featVec = np.linalg.eig(self.covX)  # 求解协方差矩阵的特征值和特征向量
                index = np.argsort(-self.featValue)  # 依照featValue进行从大到小排序
                selectVec = np.matrix(self.featVec[:, index[:self.k]])
                finalData = data_adjust * selectVec  # 正常pca
        return finalData
    def ctbr_single(self,x_index):
        index = np.argsort(-self.featValue)
        ctb_r=self.featValue[index[x_index]]/self.featValue.sum()
        return ctb_r
    def ctbr(self):
        index = np.argsort(-self.featValue)
        ctb_r=self.featValue[index[:self.k]].sum()/self.featValue.sum()
        return ctb_r
    def ctbrmat(self):
        m,n=self.x.shape
        index = np.argsort(-self.featValue)
        c_mat=a = [[0 for i in range(self.k)] for j in range(n)]

        for i in range(n):#n=5
            for j in range(self.k):#k=3
                c_mat[i][j]=self.featValue[index[j]]**0.5*np.matrix(self.featVec).T[j,i]/self.covX[i,i]**0.5
        index=['x'+str(i) for i in range(n)]
        columns=['y'+str(i) for i in range(self.k)]
        c_df = pd.DataFrame(c_mat,index=index,columns=columns)

        return c_df
```

### 与sklearn对比


```python
df = pd.read_excel(r'C:\Users\Administrator\PycharmProjects\try\美国各州犯罪情况.xlsx', engine='openpyxl')
df = df.reset_index(drop=True)
df = df.iloc[:,1:]
#df = pd.DataFrame(np.random.rand(10,5))
print(df.head())

```

        杀人罪   强奸罪    抢劫罪    伤害罪     夜盗罪     盗窃罪   汽车犯罪
    0  14.2  25.2   96.8  278.3  1135.5  1881.9  280.7
    1  10.8  51.6   96.8  284.0  1331.7  3369.8  753.3
    2   9.5  34.2  138.2  312.3  2346.1  4467.4  439.5
    3   8.8  27.6   83.2  203.4   972.6  1862.1  183.4
    4  11.5  49.4  287.0  358.0  2139.4  3499.8  663.5



```python
sklearn_pca = sklearnPCA(n_components=3)
sklearn_transf0 = sklearn_pca.fit_transform(df)
print(sklearn_transf0[0:10])
```

    [[-771.15048775  234.25236739  -95.08157407]
     [ 676.81479101 -172.58680792  358.53560434]
     [2072.70622533   -8.31181716 -213.87526248]
     [-881.24116568   70.03811734 -119.48000834]
     [1170.44830923  404.93627756   14.68690359]
     [1394.36013983  -40.15480702  -64.64091291]
     [   2.71726054  124.73620596  188.88141502]
     [1072.59455392 -150.49354577   13.46867697]
     [1299.54221779  -79.89297865 -179.33295523]
     [-415.83813214  271.53224968 -128.5181163 ]]



```python
pca = PCA(3,df,'cov')
x = pca.fit()
print(x[0:10])
```

    [[  771.15048775   234.25236739   -95.08157407]
     [ -676.81479101  -172.58680792   358.53560434]
     [-2072.70622533    -8.31181716  -213.87526248]
     [  881.24116568    70.03811734  -119.48000834]
     [-1170.44830923   404.93627756    14.68690359]
     [-1394.36013983   -40.15480702   -64.64091291]
     [   -2.71726054   124.73620596   188.88141502]
     [-1072.59455392  -150.49354577    13.46867697]
     [-1299.54221779   -79.89297865  -179.33295523]
     [  415.83813214   271.53224968  -128.5181163 ]]


### 此处第一列符号不同是由于sklearn里使用svd求解，符号有所不同，具体参见  <https://zhuanlan.zhihu.com/p/85701151>

### 使用相关系数矩阵进行pca


```python
pca = PCA(3,df,'coeff')
x = pca.fit()
print(x[0:10])
```

    [[ -361.97619967   426.36619157   386.60137538]
     [  408.23686513  -445.29198009  -195.73340166]
     [ 1174.09659105  -931.27972649 -1156.29139046]
     [ -505.1662355    488.1952512    373.08030324]
     [  887.7999747   -599.26088532  -396.57559077]
     [  810.32263389  -648.35889528  -729.45105074]
     [   35.17565238  -130.10025985   148.32081874]
     [  563.76332822  -537.89281093  -556.76337833]
     [  786.58674871  -489.68804364  -752.32935396]
     [ -148.48137674   247.98734282   216.96182286]]


### 查看第一个主成分的贡献率


```python
pca = PCA(3,df,'cov')
x=pca.fit()
x = pca.ctbr_single(0)
print(x)
```

    0.8735949017216013


### 查看前三个主成分的贡献率


```python
x = pca.ctbr()
print(x)
```

    0.9878157218362371


### 查看贡献值矩阵


```python
z=pca.ctbrmat()
print(z)
```

              y0        y1        y2
    x0 -0.183188  0.461760 -0.296795
    x1 -0.668448  0.269126 -0.150410
    x2 -0.528858  0.473844  0.194281
    x3 -0.484077  0.438520 -0.233630
    x4 -0.882167  0.451833 -0.124327
    x5 -0.985782 -0.167456  0.012799
    x6 -0.514558  0.432815  0.738388

